---
date: 2019-09-14 08:07:00
status: public
categories:
 - deep learning
tags: 
 - 深度学习
title: bert的基本使用
---



本文试图回答的问题：

* bert的使用过程是什么样的
* bert的输入输出是什么样的
* 在使用bert的过程中有哪些关键点



## 源码下载

```
!git clone https://github.com/google-research/bert.git
```



## 下载bert中文预训练模型文件

```bash
# 下载
!wget https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip

# 解压
!unzip chinese_L-12_H-768_A-12.zip
```



## 定义模型

```python
import tensorflow as tf
from bert import modeling
```

```python
max_length = 10
input_ids = tf.placeholder(tf.int64, shape=[bash_size, max_length])
input_mask = tf.placeholder(tf.int64, shape=[bash_size, max_length])
token_type_ids = tf.placeholder(tf.int64, shape=[bash_size, max_length])

# 生成bert_config
""""
不使用预训练模型时也可以自己定义：
config = modeling.BertConfig(vocab_size=32000, hidden_size=512,
    num_hidden_layers=8, num_attention_heads=6, intermediate_size=1024)
"""
bert_config_file = 'chinese_L-12_H-768_A-12/bert_config.json'
bert_config = modeling.BertConfig.from_json_file(bert_config_file)

model = modeling.BertModel(
        config=bert_config,
        input_ids=input_ids,
        input_mask=input_mask,
        token_type_ids=token_type_ids,
        is_training=False,
        use_one_hot_embeddings=True)
```



## 生成测试数据

```python
from bert import tokenization
tokenizer = tokenization.FullTokenizer(
        vocab_file='chinese_L-12_H-768_A-12/vocab.txt', do_lower_case=True)


def generate_features(input_text, max_seq_length):
    tokens = tokenizer.tokenize(input_text)
    
    if len(tokens) > max_seq_length - 2:
      tokens = tokens[0:(max_seq_length - 2)]

    tokens.insert(0, '[CLS]')
    tokens.append('[SEP]')
    input_ids = tokenizer.convert_tokens_to_ids(tokens)
    while len(input_ids) < max_seq_length:
        input_ids.append(0)
    input_mask = [ 0 if input_id ==0 else 1 for input_id in input_ids]
    segment_ids = [0] * len(input_ids)

    return input_ids, input_mask, segment_ids

# 生成一个example
feature_input_ids, feature_input_mask, feature_segment_ids = generate_features('测试句子', 10)
# input_ids 字的one-hot编码，不足max_seq_length部分用0补齐
# input_mask 补齐部分为0，非补齐部分为1
# segments_ids 第一部分为0，第二部分为1
print(feature_input_ids)
print(feature_input_mask)
print(feature_segment_ids)
```

输出：

```
[101, 3844, 6407, 1368, 2094, 102, 0, 0, 0, 0]
[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]
[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
```



## 输出bert结果

```python
# [CLS] 结果全联接层后的结果
pooled_output = model.get_pooled_output()
# 整个sequence的结果包括[CLS][SEP]
sequence_output = model.get_sequence_output()

with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    pool_ret, sequence_ret = sess.run([output_layer, sequence_output], 
                                      feed_dict={
                                          input_ids: [feature_input_ids], 
                                          input_mask: [feature_input_mask], 
                                          token_type_ids:[segment_ids]})
    print(pool_ret.shape)
    print(sequence_ret.shape)
```

输出

```
# batch_size, embeding_size
(1, 768)
# batch_size, max_seq_length, embeding_size
(1, 10, 768)
```

对于句子门类等任务我们使用`pooled_output`即可，如果是序列标注则使用`sequence_output`



##  加载bert预训练模型

```python
init_checkpoint = 'chinese_L-12_H-768_A-12/bert_model.ckpt'
# 调用init_from_checkpoint方法
tvars = tf.trainable_variables()
(assignment_map, initialized_variable_names) = modeling.get_assignment_map_from_checkpoint(
    tvars, init_checkpoint)
tf.train.init_from_checkpoint(init_checkpoint, assignment_map)

## 以上代码需要在 模型定义之后，sess.run(tf.global_variables_initializer()) 调用
```

不加载bert预训练模型时我们输出一个bias变量：

```python
graph = tf.get_default_graph()
bias_var = graph.get_tensor_by_name('bert/encoder/layer_9/output/dense/bias:0')
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    bias_var_ret = sess.run(bias_var)
    print(bias_var_ret)
```

输出：

```
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.
 0. 0. 0. 0. ....]
```

加载后在来看下：

```
[ 1.59017026e-01  9.16422606e-02 -2.22222768e-02 -9.68082398e-02
  3.88148576e-02 -1.29534647e-01  1.27454013e-01  5.71930073e-02
  1.27984751e-02 -7.38708153e-02 -1.95696447e-02 -6.19950853e-02
 -8.56598467e-02 -5.69710024e-02  6.46896958e-02 -3.26531418e-02
  4.72894274e-02  4.64077713e-03 ...]
```

我们看到已经加载成功了