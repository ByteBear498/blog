---
date: 2017-07-06 23:18:00
status: public
title: 信息的度量熵
Tags: 
 - 自然语言处理
 - 熵
categories:
 - NLP
---

## 熵
一条信息的信息量与它的不确定性成正比关系。比如说，我们要搞清楚一件非常不确定的事，或是我们一无所知的事情，就需要了解大量的信息。相反，如果我们对某件事已经有了较多的了解，那么不需要太多的信息就能把它搞清楚。所以从这个角度来看，可以认为，**信息量就等于不确定性的多少**。

它的定义如下：
$$
H(X)=-\sum_{x\in X}P(x) \log P(x)
$$

## 条件熵
如果 H(Y|X=x) 为变数 Y 在 X 取特定值 x 条件下的熵，那么 H(Y|X)就是 H(Y|X=x) 在 X 取遍所有可能 x 后平均的结果：
$$
\begin{align}
H(Y|X) &= \sum_i^n P(x_i)H(Y|X=x_i) \\
&= \sum_i^n P(x_i) \sum_j^m P(y_j) \log P(y_j|x_i) \\
&= \sum_i^n \sum_j^m P(x_i,y_j) \log P(y_j|x_i) \\
&= - \sum_{x\in X,y \in Y}P(x,y) \log P(y|x) \\
\end{align}
$$
现在假定我们还知道X的一些情况，包括它和Y一起出现的概率，以及在X取不同值的前提下X的概率分布。定义在X的条件下Y的条件熵为：
$$
H(Y|X)=-\sum_{x\in X,y \in Y}P(x,y) \log P(y|x)
$$
H(Y)>=H(Y|X)，也就是说Y的不确定性下降了。

## 互信息
互信息有时也叫“信息增益”。当获取的信息和要研究的事物“有关系”时，这些信息才能帮助我们消除不确定性。当然“有关系”这种说法太模糊，太不科学，最好能度化的度量“相关性”。香农在信息论中提出了一个“互信息”的概念作为对两个随机事件“相关性”的量化度量。假定有两个随机事件X和Y，它们的互信息定义如下：
$$
I(X;Y)=\sum_{x \in X,y \in Y}P(x,y) \log \frac{P(x,y)}{P(x)P(y))}
$$
其实这个互信息就是，随机事件X的不确定性或者说熵H(X)，以及在知道随机事件Y条件下的不确定性或者说H(X|Y)之间的差异，即：
$$
I(X;Y)=H(X) - H(X|Y)
$$
也就是说，**所谓的两个事件相关性的度量，就是在了解了其中一个Y的前提下，对消除另一个X不确定性所提供的信息量**。当X和Y完全相关是，它的取值是1；当两者完全无关时，它的取值是0。

## 相对熵
相对熵，在有些文献中它也被称为“交叉熵”，在英文中是 Kullback-Leibler Divergence ，是它的两个提出者的名字命令的，简称KL散度。相对熵也用来衡量相关性，但和变量的互信息不同，它用来衡量两个取值为正数的函数的相似性，它的定义如下：
$$
KL(f(x)||g(x))=\sum_{x \in X}f(x) \log \frac{f(x)}{g(x)}
$$
相对熵有如下三条结论：
1. 对于两个完全相同的函数，它们的相对熵等于零。
2. 相对熵越大，两个函数的差异越大；反之，相对熵越小，两个函数差异越小。
3. 对于概率分布或者概率密度函数，如果取值均大于零，相对熵可以度量两个随机分布的差异性。

在自然语言处理中相对熵的应用很多，比如用来衡量两个常用词在不同文本中的概率分布，看它们是否同义；或者根据两篇文章不同词的分布，看看它们的内容是否相近等等。

另外根据相对熵可以推导出交叉熵。

## 交叉熵

将相对熵公式进行变形：

$$
\begin{align}
KL(f(x)||g(x)) &= \sum_{x \in X}f(x) \log \frac{f(x)}{g(x)} \\
&= \sum_{x \in X}f(x) \log f(x) - \sum_{x \in X}f(x) \log g(x) \\
&= -H(f(x)) - \sum_{x \in X}f(x) \log g(x)
\end{align}
$$

其中$H(f(x))$是真实值的信息熵，第二项就是多分类的交叉熵。因此KL散度也被成为相对熵。

$$
CE = - \sum_{x \in X}f(x) \log g(x)
$$





## 总结
一个事物内部会存在随机性，也就是不确定性，而从外部消除这个不确定性唯一的办法是引入信息。如果没有信息，任何公式或者数学的游戏都无法排队不确定性，这个朴素的结论非常重要。

信息的作用在于消除不确定性，自然语言处理的大量问题就是找相关的信息。
- - - -
《数学之美》