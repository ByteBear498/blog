---
date: 2021-06-00 16:18:00
title: 对LSTM的持续理解
mathjax: true
markup: mmark
Tags: 
 - 自然语言处理
categories:
 - NLP
---



深度学习的流行起源于图像领域，CNN算是最早被大家所熟悉的网络结构。但是对于带有时间特性或序列性质的数据任务上，CNN式的网络结构并不是完美契合的。因此后来有人提出了与上述任务更加契合的网络结构-RNN。

RNN实际上是一类网络结构的统称，具有以下特点的网络都可以算法是RNN。

![img](2021-06-02-%E5%AF%B9LSTM%E7%9A%84%E6%8C%81%E7%BB%AD%E7%90%86%E8%A7%A3.assets/RNN.png)

即：
$$
y^t, h^t = f(x^t, h^{t-1})
$$
因此对$f$的不同处理产生了不同的RNN算法，如LSTM、GRU。

与CNN相同，也可以对RNN进行Deep：

![img](2021-06-02-%E5%AF%B9LSTM%E7%9A%84%E6%8C%81%E7%BB%AD%E7%90%86%E8%A7%A3.assets/Deep_RNN.png)



### RNN面临的问题

回到单层的RNN，设计一个简单的$f$函数
$$
\begin{align}
h_t &= tanh(W^xx_t + W^hh_{t-1}) \\
y_t &= tanh(W^oh_t)
\end{align}
$$
上述方法有一个致命的缺点——梯度消失（梯度爆炸）。那如何解决这个问题呢，其中一种方案就是LSTM。





### LSTM的提出

LSTM的计算方式如下：
$$
\begin{align}
& i_t = sigm(W^{xi}x_t + W^{hi}h_{t-1}) \\
& f_t = sigm(W^{xf}x_t + W^{hf}h_{t-1}) \\
& o_t = sigm(W^{xo}x_t + W^{ho}h_{t-1}) \\
& \tilde{c_t} = tanh(W^{xc}x_t + W^{hc}h_{t-1}) \\
& c_t = f_t \bigodot c_{t-1} + i_t \bigodot \tilde{c_t} \\
& h_t = o_t \bigodot tanh(c_t) \\
& y_t = \sigma(W^o h_t)
\end{align}
$$




#### LSTM如何解决梯度消失问题



#### 如何理解 hidden state(h) 和 cell state(c)



##### 说法1

hidden state是cell state经过一个神经元和一道“输出门”后得到的，因此hidden state里包含的记忆，实际上是cell state衰减之后的内容。另外，cell state在一个衰减较少的通道里沿时间轴传递，对时间跨度较大的信息的保持能力比hidden state要强很多。

因此，实际上hidden state里存储的，主要是“近期记忆”；cell state里存储的，主要是“远期记忆”。cell state的存在，使得LSTM得以对长依赖进行很好地刻画。

##### 说法2

h(t) 是依赖于 c(t) 的非线性变换。

c(t) 是使用 f(t) 和 i(t) 计算的。

最后 f (t) 和 i(t) 取决于 h(t−1)。

因此，在我看来，c 充当内存，而 h 充当它的副本，可以传递它以在下一个时间步进一步处理输入。 也许这可能是 GRU 将两者合并的动机。



---

https://www.zhihu.com/question/68456751/answer/1097053422
