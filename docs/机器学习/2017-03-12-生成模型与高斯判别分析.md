---
date: 2017-03-12 11:09:00
status: public
title: 生成模型与高斯判别分析
categories:
 - machine learning
---

## 生成模型与判别模型
###  判别模型
判别模型对P(y|x)或者说对y建模，直接学习得到y。
常见的判别模型有线性回归、对数回归、线性判别分析、支持向量机、boosting、条件随机场、神经网络等。

###  生成模型
生成模型对P(x|y)或者说对x建模，通过计算
$$
\begin{aligned}
\arg\max\limits_{y}p(y\vert x) &= \arg\max\limits_y\frac{p(x\vert y)p(y)}{p(x)} \\
&= \arg\max\limits_y p(x\vert y)p(y)
\end{aligned}
$$
得到y。
因此给定x进行比较时，P(x)为固定值，所以P(x)可省略。
常见的生产模型有隐马尔科夫模型、朴素贝叶斯模型、高斯混合模型、LDA、Restricted Boltzmann Machine等。

---

## 高斯分布
若随机变量 X, X服从一个位置参数为 $\mu$ 、尺度参数为$\sigma$ 的概率分布，记为：
$$
\displaystyle X \sim N(\mu ,\sigma ^{2})
$$
则其概率密度函数为
$$
\displaystyle f(x)={1 \over \sigma {\sqrt {2\pi }}}\,e^{-{(x-\mu )^{2} \over 2\sigma ^{2}}}
$$

## 多元高斯分布
$$
X ∼ N(\mu,\Sigma)
$$

$$
\begin{aligned}
E[X] &= \mu \\
Cov(X) &= \Sigma
\end{aligned}
$$
概率密度函数为 
$$
\begin{equation}
p(x;\mu,\Sigma) = \frac{1}{(2\pi)^{\frac{n}{2}}\big\vert\Sigma\big\vert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)}
\end{equation}
$$
其中$|\Sigma|$是$\Sigma$的行列式，$\Sigma$是协方差矩阵，而且是对称半正定的。

---

## Joint似然与Condition似然
定义Condition似然
$$
L(\theta) = \prod_{i=1}^{m}P(y^{(i)}|x^{(i)})
$$

定义Joint似然
$$
L(\theta) = \prod_{i=1}^{m}P(x^{(i)},y^{(i)})
$$

---

## 高斯判别模型
高斯判别分析模型假设P(X|Y)服从多元高斯分布,并且y本身是服从贝努力分布。

$$
\begin{aligned}
y &\sim Bernoulli(\phi) \\
x\big\vert y &\sim \cal{N}(\mu_0,\Sigma) \\
x\big\vert y &\sim \cal{N}(\mu_1,\Sigma)
\end{aligned}
$$
>  意这里的参数有两个$\mu$，表示在不同的结果模型下，特征均值不同，但我们假设协方差$\Sigma$相同。反映在图上就是不同模型中心位置不同，但形状相同。这样就可以用直线来进行分隔判别。

展开
$$
\begin{aligned}
p(y) &= \phi^y(1-\phi)^{1-y} \\
p(x\vert y = 0) &= \frac{1}{(2\pi)^{\frac{n}{2}}\big\vert\Sigma\big\vert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)} \\
p(x\vert y = 1) &= \frac{1}{(2\pi)^{\frac{n}{2}}\big\vert\Sigma\big\vert^{\frac{1}{2}}}e^{-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)}
\end{aligned}
$$

joint对数似然函数为
$$
\begin{aligned}
\ell(\phi,\mu_0,\mu_1,\Sigma) &= \log\prod\limits_{i=1}^mp(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma) \\
&= \log\prod\limits_{i=1}^mp(x^{(i)}\vert y^{(i)};\phi,\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)
\end{aligned}
$$

极大化joint对数似然函，经过推导可得：
$$
\begin{aligned}
\phi &= \frac{1}{m}I\{y^{(i)}=1\} \\
\mu_0 &= \frac{\sum\limits_{i=1}^m1\{y^{(i)}=0\}x^{(i)}}{\sum\limits_{i=1}^m1\{y^{(i)}=0\}} \\
\mu_1 &= \frac{\sum\limits_{i=1}^m1\{y^{(i)}=1\}x^{(i)}}{\sum\limits_{i=1}^m1\{y^{(i)}=1\}} \\
\Sigma &= \frac{1}{m}\sum\limits_{i-1}^m(x^{(i)} - \mu_{y^{(i)}})(x^{(i)} - \mu_{y^{(i)}})^T
\end{aligned}
$$

$\phi$是训练样本中结果y=1占有的比例。

$\mu_0$是y=0的样本中特征均值。

$\mu_0$是y=1的样本中特征均值。

$\Sigma$是样本特征方差均值。

## Logistic回归与高斯判别模型
当x|y符合高斯分布，那么P(y=1|x)就是logsitic回归。但是反过来如果P(y=1|x)是logsitic回归，x|y不一定是高斯分布。例如当x|y符合泊松分布，那么P(y=1|x)也会是logsitic回归。

相比Logistic，高斯判别模型有更强的假设，即假设x|y符合高斯分布。也因此高斯判别模型需要更少的数据。

如果x|y=0,x|y=1,符合指数分布簇，P(y=1|x)都会是logsitic回归。

所以高斯判别模型的优点是所需要的数据少，但有很强的假设鲁棒性不高。
反之Logistic的优点鲁棒性高，但所需要的数据相对多。