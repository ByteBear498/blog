---
date: 2017-03-12 11:05:00
status: public
title: 朴素贝叶斯
tag: 
 - 机器学习 
 - 分类算法 
 - 有监督 
 - 生成模型
categories:
 - machine learning
---
朴素贝叶斯是一种分类模型，属于生成模型。比较常见的应用是垃圾邮件分类；

在垃圾邮件识别为例

$$
y\in\{0,1\}
$$
y=1表示垃圾邮件

将每封邮件表示成一个向量，向量中每一维都是字典中的一个词的0/1值，1表示该词在邮件中出现，0表示未出现。
假设有一个50000个词的词表，一封邮件x可表示（多元伯努利时间模型）
$$
x=
\begin{bmatrix}
0 \\
1 \\
1 \\
… \\
\end{bmatrix}
$$

$$
x=\in\{0,1\}^n
$$
n=50000

## 假设
我们还是对P(X|Y)建模

$$
\begin{aligned}
& P(x_1,x_2,...x_{50000}|y) \\
&= P(x_1|y)*P(x_2|y,x_1)*P(x_3|y,x_1,x_2)... \\
&= P(x_1|y)*P(x_2|y)*P(x_3|y)...P(x_50000|y)  \\
&= \prod_{i=1}^{m}P(x_i|y)
\end{aligned}
$$
NLP中的n元语法模型有点类似，这里相当于一元文法unigram。

这里有个假设，条件独立性假设，形式化表示为，（如果给定Z的情况下，X和Y条件独立）

这里我们发现朴素贝叶斯假设是约束性很强的假设，“buy”从通常上讲与“price”是有关系，我们这里假设的是条件独立。

## 模型参数
$$
\begin{aligned}
\phi_{i|y=1} &= P(x_i=1|y=1) \\
\phi_{i|y=0} &= P(x_i=1|y=0) \\
\phi_y&=P(y=1)
\end{aligned}
$$

joint似然函数为：
$$
L(\phi_y,\phi_{i|y=0},\phi_{i|y=1})= \prod_{i=1}^{m}P(x^{(i)},y^{(i)})
$$

求该 Joint似然最大化得：
$$
\begin{aligned}
\phi_{i|y=1} &= \frac{\sum_{i=1}^mI\{x_j^{(i)}=1,y^{(i)}=1\}}{\sum_{i=1}^mI\{y^{(i)}=1\}} \\
\phi_y &= \frac{\sum_{i=1}^mI\{y^{(i)}=1\}}{m}
\end{aligned}
$$
二个式子分别表示在y=1或0的样本中，特征Xj=1的比例和y=1的样本数占全部样本数的比例。

---

## 拉普斯平滑
假设一个单词从未出现过，例如该词是第3500个词，也就是
$$
P(x_{3500}|y=0)=0
P(x_{3500}|y=1)=0
$$
当计算
$$
P(y=1|x) = \frac{P(x|y=1)*P(y=1)}{P(x|y=1)*P(y=1) + P(x|y=0)*P(y=0)}
$$
又因为,$P(x_{3500}|y=1) = 0$，因此
$$
P(x|y=1)*P(y=1) = \prod_{i=1}^{m}P(x_i|y) = 0
$$
所以，之前的等式
$$
P(y=1|x) = \frac{P(x|y=1)*P(y=1)}{P(x|y=1)*P(y=1) + P(x|y=0)*P(y=0)} = \frac{0}{0+0}
$$
这里存在的问题是认为$P(x_{3500}|y=1)=0$不是个好主意。可以认为一件事不太可能发生，但不能认为不会发生。

那么拉普斯
$$
P(y=1) = \frac{\#"1"}{\#"1" + \#"0"} = \frac{\#"1" + 1}{\#"1" + 1 + \#"0" + 1}
$$

如果$y \in \{1,2,3...k\}$，那么
$$
P(y=j) = \frac{\sum_{j=1}^m I(y^{(i)}=j)}{m}
$$
对它使用拉普斯平滑
$$
P(y=j) = \frac{\sum_{j=1}^m I(y^{(i)}=j) + 1}{m+k}
$$

将拉普斯平滑应用到朴素贝叶斯，就是
$$
\begin{aligned}
\phi_{i|y=1} &= \frac{\sum_{i=1}^mI\{x_j^{(i)}=1,y^{(i)}=1\}+1}{\sum_{i=1}^mI\{y^{(i)}=1\}+2}
\end{aligned}
$$

---

## 朴素贝叶斯事件模型
另一种将文档表现为特征向量的方式
$$
x^{(i)}={x^{(i)}_1,x^{(i)}_2,x^{(i)}_2 ... x^{(i)}_{n_i}}
$$
ni为文档的总词数，$x_j \in \{1,2,3 ... 50000\}$为词典中的索引。那么
$$
P(x,y) = \left( \prod_{i=1}^{n}P(x_i|y) \right) P(y)
$$
在这里，认为写邮件时，先选择了邮件的类型（是否为垃圾邮件）然后按照一定的比例选择用词来生成文档。所以是生成模型。

### 模型参数
$$
\phi_{k|y=1}=P(x_i=k|y=1) \\
\phi_{k|y=1}=P(x_i=k|y=0)  \\
\phi_y=P(y=1)
$$
第一个式表示在给定y=1的情况下，选择词K的概率。

对这些参数进行极大似然估计，
$$
\begin{aligned}
& l(\phi_{k|y=1},\phi_{k|y=1},\phi_y)  \\
& = \log \prod_i^m P(x^{(i)},y^{(i)};\phi_{k|y=1},\phi_{k|y=1},\phi_y)  \\
& = \log \prod_i^m \prod_{j=1}^{n_i}P(x^{(i)}_j|y^{(i)};\phi_{k|y=1},\phi_{k|y=1}) * P(y^{(i)};\phi_y)
\end{aligned}
$$
可得
$$
\phi_{k|y=1} = \frac{\sum_{i=1}^m I(y^{(i)}=1) \sum_j^{n_i} I(x_j^{(i)}=k)}{\sum_{i=1}^m I(y^{(i)}=1)*n_i}
$$
分子的意思是，对所有垃圾邮件中词K出现的次数。分母的意思是，所有垃圾邮件的总词数。
如果对上面的式子进行拉普斯平滑的话则为
$$
\phi_{k|y=1} = \frac{\sum_{i=1}^m I(y^{(i)}=1) \sum_j^{n_i} I(x_j^{(i)}=k)+1}{\sum_{i=1}^m I(y^{(i)}=1)*n_i+50000}
$$

其实拉普斯是
$$
x \in \{ 1,2,3...l \}\\
P(x=k) = \frac{\#"k" + 1}{\#"1" + \#"2" + \#"3" + ... + \#"l" + l}
$$

---

## 极大似然估计的推导
假设
$$
y \in \{ 1,2,3...k \} \\
$$
那么
$$
P(y)=\prod_{1}^K P(y=c_k)^I(y=c_k)
$$
当y=ck，文档x的概率为：
$$
\begin{aligned}
P(x|y=c_k)=\prod_{j=1}^{n_i} P(x_j|y=c_k)
\end{aligned}
$$
$$
\begin{aligned}
& l(\phi_{k|y=1},\phi_{k|y=1},\phi_y)  \\
& = \log \prod_i^m P(x^{(i)},y^{(i)};\phi_{k|y=1},\phi_{k|y=1},\phi_y)  \\
& = \log \prod_i^m \prod_{j=1}^{n_i}P(x^{(i)}_j|y^{(i)};\phi_{k|y=1},\phi_{k|y=1}) * P(y^{(i)};\phi_y) \\
& =
\end{aligned}
$$