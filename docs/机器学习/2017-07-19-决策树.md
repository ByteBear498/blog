---
date: 2017-07-20 21:00:00
status: public
title: 决策树
tags: 
 - 机器学习
categories:
 - machine learning
---
## 背景
决策树的优点是计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。缺点是可能会产生过度匹配问题。适用于连续值和离散值数据。

## 决策树生成
训练集$D=\{(x_1,y_1),(x_2,y_2),\dots,(x_m,y_m)\}$
属性集$A=\{a_1,a_2,\dots,a_d\}$

```
def function treeGenerate(D,A):
    生成结node;
    if Dv为空:
        将该分支结点标记为叶结点，其类别标记为D中样本最多的类;return;
    if D中所有实例属于同一类:
        将该分支结点标记为叶结点，将该类作为其类别标记;return;
    a* = 从A中选择最优划分属性;
    for v in a*:
        为node生成一个分支；
        令Dv表示D中在a*上取值为v的样本子集；
        以treeGenerate(Dv,A-a*)为分支结点
```

## 特征选择
特征选择在于选取对训练数据具有分类能力的特征。这样可以提高决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有特征是没有分类能力的。

### 信息增益
信息增益也叫互信息。当获取的信息和要研究的事物“有关系”时，这些信息才能帮助我们消除不确定性。当然“有关系”这种说法太模糊，太不科学，最好能度化的度量“相关性”。
熵：
$H(X) = -\sum_i^nP(x_i)\log(P(x_i)$
条件熵：
$H(Y|X) = \sum_i^nP(x_i)H(Y|X=x_i)$
信息增益就是熵与条件熵的差值：
$$
Gain(D,a) = H(D) - H(D|a) = -\sum_i^{|Y|} P(Y_i) \log P(Y_i) - \sum_j^{|a|}P(a_j)H(D|a=a_j)  \\
= H(D) - \sum_{v=1}^V \frac{|D^v|}{|D|}H(D^v)
$$
### 信息增益比
信息增益作为划分训练数据的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行校正。
信息增益比：
$$
Gain_ratio(D,a) = \frac{Gain(D,a)}{H(a)} \\
= \frac{H(D) - H(D|a)}{H(a)} \\
=  \frac{Gain(D,a)}{-\sum_{v=1}^V  \frac{|D^v|}{|D|} \log \frac{|D^v|}{|D|}} 
$$

### 总结
除了信息增益与信息增益比，特征的选择还可以使用基尼指数；另外决策树生成的过程中，**剪枝**的操作也很重要，可以防止模型的过拟合；决策树了除了可以用于分类也适用于回归任务，当然方式也会有相应不同；

